After researching on how to implement a speech-to-text system, I identified two major stages that this task should be split into: taking in audio inputs and outputting the audio inputs into a text. 

During the research, I found various approaches for the first stage, such as extracting the audio from a YouTube video and recording an input audio directly on the device that will also run the speech-to-text algorithm. However, not only I had some confusion comprehending such methods, my attempt on implementing similar algorithms failed to run on my Google Colab. I eventually resolved the issue by uploading a WAV file into a Google Drive folder and importing Google Drive to Colab, so that I can directly call the file in my code. 

The audio input I selected is a song titled “10 Years Later” from the soundtrack of the musical Come From Away. Although length was the criteria I put greatest significance in when selecting the audio input, this 17-second-long song has several peculiar traits that offer an interesting challenge to the speech-to-text system. Despite being part of a musical soundtrack, there is no actual singing in this song; it is composed of only narrations and melodies. I wanted to see whether my implementation could “hear through” the melody and catch lyrics. In addition, there are several acronyms and proper nouns featured in the track, which would raise the level of difficulty to transcribe. 

Once I successfully access the audio file within my algorithm, I could preprocess audio and extract features. I decided to use Wav2Vec2, the successor to Wav2Vec which was mentioned in the task explanation section from the application instruction. 

Frankly speaking, even though the task explanation stated to not use external libraries for ASR such as Speech Recognition and google-speech, I was not certain which python packages or libraries I could use and could not for this task. For my implementation, I imported torch (to derive prediction using torch.argmax), librosa (to load an input audio file), IPython (to make sure the correct audio input is imported using IPython.display.Audio), and transformers (to load pre-trained model using Wav2Vec2Processor and Wav2Vec2ForCTC). 

From the research, I learned that the Wav2Vec2 model is trained with connectionist temporal classification (CTC), so the output of the model has to be decoded with Wav2Vec2CTCTokenizer. So I am confident that I imported appropriate python libraries.

I first used IPython.display.Audio to make sure the “10 Years Later” audio file was imported successfully. Then I used librosa to load the input audio file that is resampled to a 16 kHz frequency, which is the sampling rate the Wav2Vec2 model is pre-trained on. After that, I imported a pre-trained Wav2Vec2 model using Wav2Vec2Processor. 

But from the research, I also learned that there are various kinds of pre-trained speech to text models that can be loaded to the Wav2Vec2Processor, so I wanted to take half a step further and compare the outputs derived from two different models. I selected “facebook/hubert-large-ls960-ft” and “facebook/wav2vec2-base-960h”, and from this stage of loading pre-trained model, I implemented pairs of code blocks that are performing identical tasks for different “variables” derived from different pre-trained model. 

Then for each model, I took input values and passed the audio array into the tokenizer to derive tensors in PyTorch format (which was made possible by return_tensors="pt" command). Passing logit values to torch.argmax outputted the prediction that can be passed to the tokenizer and decoded, which can be ultimately printed into a transcript. 

As mentioned earlier, I wanted to compare the transcripts derived from two different pre-trained models. I found out that “facebook/hubert-large-ls960-ft” outputted a transcript with much higher accuracy than that from “facebook/wav2vec2-base-960h”. In the latter’s defense, the first half of “10 Years Later” is where most of the proper nouns and acronyms are clustered in, and its transcription for the second half of the song is much more accurate. However, even when the Wav2Vec2Processor with “facebook/hubert-large-ls960-ft” loaded in output typos, they are much closer to the correct spelling than that of the other model. 
